
# LinguisticNode

LinguisticNode is a vocabulary learning and spaced-repetition memory app, featuring a FastAPI backend and a React (TypeScript) frontend. The project is fully containerized with Docker.

## Project Structure

```
app/
  __init__.py
  main.py              # FastAPI app creation and middleware
  settings.py          # Configuration and environment variables
  models.py            # Pydantic models for API and data
  security.py          # Password hashing and validation
  sessions.py          # Session management
  storage.py           # File-based data storage
  services.py          # Business logic layer
  deps.py              # Dependency injection (auth, etc.)
  routers/             # API endpoint modules
    __init__.py
    auth.py            # Authentication endpoints
    words.py           # Word CRUD endpoints
    study.py           # Spaced-repetition study endpoints
    io.py              # Import/export endpoints
  middleware.py        # Request logging
  middleware_bodylog.py
  errors.py            # Error handling utilities
  logging_setup.py     # Structured logging configuration

tests/                 # Test suite (pytest)
  __init__.py
  conftest.py          # Test fixtures and configuration
  test_auth.py         # Authentication tests
  test_words.py        # Word management tests
  test_study.py        # Study flow tests
  test_io.py           # Import/export tests

data/                  # User data (not tracked by git)
  users/
    users.json
  vault/
    u_<userId>/
      words.json
      memory.json
      settings.json

frontend/src/          # React frontend
  api/                 # API client modules
  auth/                # Authentication context
  pages/               # Page components
  components/          # Reusable UI components

scripts/
  bump_version.sh      # Version management script

docker-compose.yml     # Container orchestration
Dockerfile             # Backend container definition
requirements.txt       # Python dependencies
pytest.ini             # Pytest configuration
```

## Setup & Build

```sh
cd /work/project/03.LexiMemory
./scripts/bump_version.sh && sudo docker compose up -d --build
```

## API Endpoints

All API endpoints are prefixed with `/api`:

- `GET /healthz` - Health check (no auth required)
- `POST /api/auth/register` - Register new user
- `POST /api/auth/login` - User login
- `POST /api/auth/logout` - User logout
- `GET /api/auth/me` - Get current user info
- `GET /api/words` - List user's words
- `POST /api/words` - Create new word
- `PUT /api/words/{id}` - Update word
- `DELETE /api/words/{id}` - Delete word
- `GET /api/study/next` - Get next card for review
- `POST /api/study/grade` - Grade a card (again/hard/good/easy)
- `GET /api/io/export` - Export user data
- `POST /api/io/import` - Import user data (overwrite/merge)

## Testing

Run the test suite:

```sh
# Install dependencies (if not using Docker)
pip install -r requirements.txt

# Run all tests
pytest

# Run specific test file
pytest tests/test_auth.py

# Run with verbose output
pytest -v

# Run with coverage report
pytest --cov=app --cov-report=html
```

## API Health Check

```sh
curl http://localhost:8000/healthz
```

The `/healthz` endpoint returns API status, app version, and git commit hash.

## Features

- **FastAPI Backend**: Modern async Python framework with automatic OpenAPI docs
- **User Authentication**: Secure password hashing with Argon2, session-based auth
- **Vocabulary Management**: Full CRUD operations for vocabulary entries
- **Spaced Repetition**: SM-2 algorithm for optimized learning
- **Data Isolation**: Per-user data storage with file-based persistence
- **Import/Export**: Backup and restore user data in JSON format
- **Structured Logging**: Request correlation, audit trails, observability
- **Comprehensive Tests**: Unit and integration tests with pytest
- **Docker Support**: Containerized deployment with docker-compose



## Staging Environment (pre-production)

æœ¬ç•ªå‰æ¤œè¨¼å‘ã‘ã« `docker-compose.stg.yml` ã¨é‹ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’è¿½åŠ ã—ã¦ã„ã¾ã™ã€‚

### ä¸€å›ã ã‘ã‚„ã‚‹äº‹å‰æº–å‚™

1. stg ç”¨ env ã‚’ä½œæˆ

```sh
cp .env.sample .env.stg
```

2. `.env.stg` ã‚’ç·¨é›†ï¼ˆå¿…é ˆï¼‰

- `PASSWORD_PEPPER` ã‚’å®‰å…¨ãªå€¤ã¸å¤‰æ›´
- å¿…è¦ã«å¿œã˜ã¦ `STG_API_PORT` / `STG_WEB_PORT` / `STG_HOST_DATA_DIR` ã‚’èª¿æ•´
- `STG_HOST_DATA_DIR` ãŒå­˜åœ¨ã—ãªã„å ´åˆã€`envctl.sh stg up/build` å®Ÿè¡Œæ™‚ã«è‡ªå‹•ä½œæˆã•ã‚Œã¾ã™

3. åˆå›ãƒ“ãƒ«ãƒ‰ï¼ˆä»»æ„: `up` ã« `--build` ãŒå«ã¾ã‚Œã‚‹ãŸã‚çœç•¥å¯èƒ½ï¼‰

```sh
./scripts/envctl.sh stg build ./.env.stg
```

### æ¯å›ã‚„ã‚‹ãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †ï¼ˆstgï¼‰

```sh
# 1) Deploy (build + up)
./scripts/envctl.sh stg up ./.env.stg

# 2) Health / Status
./scripts/envctl.sh stg ps ./.env.stg

# 3) Logs
./scripts/envctl.sh stg logs ./.env.stg

# 4) Error-only logs (last 30 minutes, regex filtered)
./scripts/envctl.sh stg errors ./.env.stg
# or compatibility wrapper
./scripts/stg_errors.sh ./.env.stg

# 5) Endpoint probe (HTTP/HTTPS headers + /api redirect check)
./scripts/envctl.sh stg probe ./.env.stg nolumia.com
# or compatibility wrapper
./scripts/stg_probe.sh ./.env.stg nolumia.com
```

### æ¯å›ã‚„ã‚‹åœæ­¢æ‰‹é †ï¼ˆstgï¼‰

```sh
./scripts/envctl.sh stg down ./.env.stg
```

### æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤æ‰‹é †ï¼ˆæ—¢å­˜é‹ç”¨ï¼‰

æœ¬ç•ªã¯ã“ã‚Œã¾ã§ã©ãŠã‚Š `prod` ã‚’æŒ‡å®šã—ã¦é‹ç”¨ã§ãã¾ã™ã€‚

```sh
# build
./scripts/envctl.sh prod build

# deploy
./scripts/envctl.sh prod up

# logs
./scripts/envctl.sh prod logs
```

### åŒä¸€ã‚¤ãƒ¡ãƒ¼ã‚¸æ˜‡æ ¼ï¼ˆæ¨å¥¨ï¼‰

æœ¬ç•ªå“è³ªã‚’é«˜ã‚ã‚‹è¦³ç‚¹ã§ã¯ã€**stg ã§æ¤œè¨¼ã—ãŸåŒä¸€ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ prod ã¸æ˜‡æ ¼**ã™ã‚‹é‹ç”¨ãŒæ¨å¥¨ã§ã™ã€‚

```sh
./scripts/promote_same_image.sh <api_image> <web_image> <from_tag> <to_tag>
```

ä¾‹: `stg-20260227` ã‚’ `prod-20260227` ã¸ã‚¿ã‚°æ˜‡æ ¼ã€‚

### è£œè¶³

- æ—¢å®šãƒãƒ¼ãƒˆï¼ˆstgï¼‰
  - API: `http://localhost:18000`
  - Web: `http://localhost:18080`
- stg Web ã‚³ãƒ³ãƒ†ãƒŠã¯ `18080` ã§ **HTTPå¾…å—** ã§ã™ã€‚`https://<host>:18080` ã¯ TLS ãƒãƒ³ãƒ‰ã‚·ã‚§ã‚¤ã‚¯ä¸ä¸€è‡´ã§ `ERR_SSL_PROTOCOL_ERROR` ã«ãªã‚Šã¾ã™ã€‚
- `/api` ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã§ `307/308` ãŒè¿”ã‚‹å ´åˆã¯ã€ä¸Šæµã‚¢ãƒ—ãƒªã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥æ­£è¦åŒ–ã«ã‚ˆã‚‹ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã§ã™ï¼ˆ`/api/` ã‚’åˆ©ç”¨ã—ã¦ãã ã•ã„ï¼‰ã€‚
- `docker-compose.stg.yml` ã§ã¯ stg ç”¨ API ã‚³ãƒ³ãƒ†ãƒŠã« `linguisticnode-api` ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ä»˜ä¸ã—ã€æ—¢å­˜ `nginx.conf` ã® upstream è¨­å®šã‚’å†åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚
- æ—¢å­˜ã® `stg_up.sh / stg_logs.sh / stg_down.sh / build_env.sh` ã¯äº’æ›ãƒ©ãƒƒãƒ‘ãƒ¼ã¨ã—ã¦åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
- `Database Initialization Error: crypto.randomUUID is not a function` ãŒå‡ºã‚‹å ´åˆã€ãƒ–ãƒ©ã‚¦ã‚¶å®Ÿè¡Œç’°å¢ƒãŒ `randomUUID` éå¯¾å¿œã§ã‚‚ã€ã‚¢ãƒ—ãƒªå´ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ UUID ç”Ÿæˆã¸è‡ªå‹•åˆ‡æ›¿ã•ã‚Œã¾ã™ã€‚

## VSCode Remote Debug

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã«ã¯ `.vscode/launch.json` ã¨ `.vscode/tasks.json` ã‚’è¿½åŠ æ¸ˆã¿ã§ã™ã€‚

- `Backend: FastAPI (uvicorn)` : FastAPI ã‚’ VSCode ã‹ã‚‰ç›´æ¥ãƒ‡ãƒãƒƒã‚°å®Ÿè¡Œ
- `Backend: Attach (debugpy :5678)` : ã™ã§ã« `debugpy` ã§èµ·å‹•ã—ãŸãƒªãƒ¢ãƒ¼ãƒˆ Python ãƒ—ãƒ­ã‚»ã‚¹ã¸ã‚¢ã‚¿ãƒƒãƒ
- `Frontend: Chrome` : Vite ã‚’èµ·å‹•ã—ã¦ Chrome ãƒ‡ãƒãƒƒã‚°
- `Fullstack: Backend + Frontend` : ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚’åŒæ™‚ãƒ‡ãƒãƒƒã‚°

### Attach ãƒ¢ãƒ¼ãƒ‰åˆ©ç”¨ä¾‹ï¼ˆãƒªãƒ¢ãƒ¼ãƒˆãƒ—ãƒ­ã‚»ã‚¹ï¼‰

```sh
python -m debugpy --listen 0.0.0.0:5678 --wait-for-client -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

ä¸Šè¨˜ã‚³ãƒãƒ³ãƒ‰ã§å¾…å—å¾Œã€VSCode ã® `Backend: Attach (debugpy :5678)` ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

### Docker ã§ã®ãƒªãƒ¢ãƒ¼ãƒˆãƒ‡ãƒãƒƒã‚°

`docker-compose.debug.yml` ã‚’è¿½åŠ ã—ã€API(debugpy:5678) ã¨ Frontend(Vite:5173) ã®é–‹ç™ºå‘ã‘ã‚³ãƒ³ãƒ†ãƒŠã‚’åˆ†é›¢ã—ã¾ã—ãŸã€‚

```sh
# Debug containers up
docker compose -f docker-compose.debug.yml up --build

# Debug containers down
docker compose -f docker-compose.debug.yml down
```

VSCode ã§ã¯ `Docker: Attach FastAPI (debugpy)` ã‚’ä½¿ã†ã¨ã€ä¸Šè¨˜ compose ã‚’ preLaunch/postDebug task ã§è‡ªå‹•èµ·å‹•ãƒ»åœæ­¢ã§ãã¾ã™ã€‚

## Development

### Running Locally (without Docker)

```sh
# Backend
cd /work/project/03.LexiMemory
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
uvicorn app.main:app --reload --port 8000

# Frontend
cd frontend
npm install
npm run dev
```

### Environment Variables

Key environment variables (see `.env.sample`):

- `VOCAB_DATA_DIR` - Data storage directory (default: `./data`)
- `VOCAB_WEB_ORIGIN` - CORS allowed origin
- `VOCAB_COOKIE_SECURE` - Use secure cookies (true in production)
- `VOCAB_SESSION_TTL_SECONDS` - Session lifetime

## Requirements

- Python 3.11+
- Node.js 18+
- Docker & Docker Compose (for containerized deployment)

## Architecture

LinguisticNode follows a clean layered architecture:

1. **API Layer** (`routers/`): HTTP endpoints, request/response models
2. **Service Layer** (`services.py`): Business logic, framework-agnostic
3. **Storage Layer** (`storage.py`): File I/O, atomic writes, user locks
4. **Security Layer** (`security.py`, `sessions.py`): Auth, password management

This separation ensures testability, maintainability, and clear boundaries.

---

For frontend details, see `frontend/README.md`.

### Frontend CI-equivalent commands

- `cd frontend && npm run lint`
- `cd frontend && npm run typecheck`
- `cd frontend && npm run test`
- `cd frontend && npm run build`
- `cd frontend && npm run test:e2e`

### Frontend routing summary

- Router: `react-router-dom` BrowserRouter (`frontend/src/App.tsx`)
- `/login`: login/register
- `/words`: word list (root redirect target)
- `/words/create`: create word
- `/words/:id`: word details
- `/study`: study flow
- `/examples`: examples testing


## Import/Export

### Export

User data can be exported as JSON for backup or data migration:

```sh
GET /api/io/export
```

**Response format:**
- `schemaVersion`: Data format version (currently 1)
- `exportedAt`: ISO 8601 timestamp
- `words`: Array of complete word entries (with IDs and timestamps)
- `memory`: Array of memory states for spaced-repetition

### Import

Data can be imported in two modes:

```sh
POST /api/io/import?mode=merge   # Default: merge with existing data
POST /api/io/import?mode=overwrite  # Replace all data
```

#### Flexible Input Format

The import endpoint accepts JSON in **two formats**:

**1. Exported data format (complete):**
- Includes all fields: `id`, `createdAt`, `updatedAt`, `dueAt`
- Perfect for restoring previous exports

**2. Manually-created format (flexible):**
- Can omit optional fields: `id`, `createdAt`, `updatedAt`, `examples`, `tags`, `memory`
- Missing IDs are auto-generated as UUIDs
- Missing timestamps are set to current time
- Perfect for quick manual data entry

#### Merge Logic (mode=merge)

When importing with merge mode, **existing words are compared by ID**:

| Situation | Behavior |
|-----------|----------|
| **New ID (not in existing data)** | âœ… Added as new word |
| **Existing ID + older timestamp** | â¸ï¸ Existing data preserved (import ignored) |
| **Existing ID + newer timestamp** | ğŸ”„ Overwritten with import data |

**Key points:**
- If you omit `id` in manual files â†’ UUID auto-generated â†’ Always added as new
- If you specify `id` â†’ Compared with existing data â†’ May merge/overwrite
- Timestamp comparison prevents accidental downgrades

#### Overwrite Logic (mode=overwrite)

Completely replaces all data:
```sh
POST /api/io/import?mode=overwrite
```

**All existing words and memory states are deleted** and replaced with import data.

#### Examples

**Minimal manual file (only required fields - guaranteed to add as new):**
```json
{
  "schemaVersion": 1,
  "words": [
    {
      "headword": "improve",
      "pos": "verb",
      "meaningJa": "æ”¹å–„ã™ã‚‹"
    }
  ]
}
```
- âœ… ID auto-generated (UUID)
- âœ… Timestamps auto-generated (current time)
- âœ… Added as new word guaranteed (no duplicates)

**File with custom ID (will merge with existing data):**
```json
{
  "schemaVersion": 1,
  "words": [
    {
      "id": "my-custom-id",
      "headword": "test",
      "pos": "verb",
      "meaningJa": "ãƒ†ã‚¹ãƒˆ"
    }
  ]
}
```
- âœ… ID explicitly specified
- âœ… Timestamps auto-generated
- âš ï¸ May merge with existing ID (timestamp decides)

**Complete exported data (all fields):**
```json
{
  "schemaVersion": 1,
  "exportedAt": "2024-01-15T10:30:45Z",
  "words": [
    {
      "id": "uuid-123",
      "headword": "improve",
      "pos": "verb",
      "meaningJa": "æ”¹å–„ã™ã‚‹",
      "memo": "é€²æ—",
      "createdAt": "2024-01-10T08:00:00Z",
      "updatedAt": "2024-01-15T09:00:00Z",
      "examples": [
        {
          "id": "ex-uuid-1",
          "en": "I want to improve my skills.",
          "ja": "ã‚¹ã‚­ãƒ«ã‚’å‘ä¸Šã•ã›ãŸã„ã€‚"
        }
      ],
      "tags": ["work", "growth"]
    }
  ],
  "memory": [
    {
      "wordId": "uuid-123",
      "easiness": 2.5,
      "interval": 10,
      "repetitions": 5,
      "dueAt": "2024-02-10T08:00:00Z"
    }
  ]
}
```
- âœ… All fields preserved
- âœ… Merge logic applies (timestamp comparison)

**Typical workflow:**
1. Export from LinguisticNode â†’ get complete format
2. Create manually â†’ use minimal format
3. Merge imports â†’ existing data + new data
4. Overwrite imports â†’ full replacement
